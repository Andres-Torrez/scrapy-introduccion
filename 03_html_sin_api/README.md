# Escenario 3 ‚Äì Scraping de HTML sin API p√∫blica

En este escenario se muestra c√≥mo utilizar **Scrapy** para extraer informaci√≥n
directamente del **HTML de una p√°gina web**, cuando **no existe una API p√∫blica** disponible.

Este es el enfoque cl√°sico del web scraping y sigue siendo muy utilizado en la pr√°ctica.

---

## üìå ¬øQu√© se aprende en este escenario?

- Qu√© hacer cuando una p√°gina no ofrece una API
- C√≥mo extraer datos directamente del HTML
- Uso de selectores CSS
- Limitaciones del scraping tradicional

---

## üõ†Ô∏è Requisitos

Antes de comenzar necesitas:

- Python 3 instalado
- Visual Studio Code
- Conexi√≥n a internet

---

## üìÅ Paso 1: Crear la carpeta del proyecto

Creamos una carpeta para este escenario y la abrimos en Visual Studio Code:

```bash
mkdir scrapy_html
cd scrapy_html
```

## üß™ Paso 2: Crear y activar el entorno virtual

Creamos un entorno virtual para aislar las dependencias del proyecto.

```bash
python -m venv venv
```

### Activar el entorno virtual

Una vez creado el entorno virtual, es necesario activarlo para que las dependencias
se instalen y ejecuten √∫nicamente dentro del proyecto.

#### En Windows
```bash
venv\Scripts\activate
```

#### En macOS o Linux
```bash
source venv/bin/activate
```

---

## üì¶ Paso 3: Instalar Scrapy

Con el entorno virtual activo, instalamos Scrapy:

```bash
pip install scrapy
```


---

## üèóÔ∏è Paso 4: Crear el proyecto Scrapy

Creamos la estructura base del proyecto usando Scrapy:

```bash
scrapy startproject productos_html
cd productos_html
```



---

## üï∑Ô∏è Paso 5: Crear el Spider

Creamos un spider que extraer√° informaci√≥n directamente desde el HTML del sitio web,
ya que no existe una API p√∫blica disponible.

```bash
scrapy genspider productos books.toscrape.com
```


## üß† Paso 6: C√≥digo del Spider (scraping HTML)

En este paso definimos la l√≥gica del spider.
Scrapy descarga el HTML de la p√°gina y extrae los datos usando selectores CSS.

```python
import scrapy

class ProductosSpider(scrapy.Spider):
    name = "productos"
    start_urls = ["https://books.toscrape.com/"]

    def parse(self, response):
        for producto in response.css("article.product_pod"):
            yield {
                "titulo": producto.css("h3 a::attr(title)").get(),
                "precio": producto.css("p.price_color::text").get()
            }

```
En este c√≥digo:

- Se accede al HTML de la p√°gina
- Se seleccionan los elementos con CSS
- Se extraen los datos visibles al usuario

---

## ‚ñ∂Ô∏è Paso 7: Ejecutar el Spider

Ejecutamos el spider desde la carpeta donde se encuentra el archivo `scrapy.cfg`:

```bash
scrapy crawl productos -o productos.json

```


---

## üìÑ Resultado

Se genera un archivo productos.json que contiene:
- Nombre del producto
- Precio
Los datos se obtienen directamente desde el HTML de la p√°gina web.

## ‚ö†Ô∏è Consideraciones importantes

- Este m√©todo es m√°s fr√°gil que usar APIs
- Cambios en el dise√±o de la p√°gina pueden romper el scraper
- Siempre se deben revisar los t√©rminos de uso del sitio web

## ‚úÖ Conclusi√≥n

Este escenario demuestra c√≥mo Scrapy puede utilizarse cuando **no existe una API p√∫blica**.
El scraping de HTML sigue siendo una t√©cnica √∫til, aunque m√°s fr√°gil, y debe usarse de forma
responsable y √©tica.


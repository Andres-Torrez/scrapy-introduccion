# Escenario 3 â€“ Scraping de HTML sin API pÃºblica

En este escenario se muestra cÃ³mo utilizar **Scrapy** para extraer informaciÃ³n
directamente del **HTML de una pÃ¡gina web**, cuando **no existe una API pÃºblica** disponible.

Este es el enfoque clÃ¡sico del web scraping y sigue siendo muy utilizado en la prÃ¡ctica.

---

## ğŸ“Œ Â¿QuÃ© se aprende en este escenario?

- QuÃ© hacer cuando una pÃ¡gina no ofrece una API
- CÃ³mo extraer datos directamente del HTML
- Uso de selectores CSS
- Limitaciones del scraping tradicional

---

## ğŸ› ï¸ Requisitos

Antes de comenzar necesitas:

- Python 3 instalado
- Visual Studio Code
- ConexiÃ³n a internet

---

## ğŸ“ Paso 1: Crear la carpeta del proyecto

Creamos una carpeta para este escenario y la abrimos en Visual Studio Code:

```bash
mkdir scrapy_html
cd scrapy_html
```

## ğŸ§ª Paso 2: Crear y activar el entorno virtual

Creamos un entorno virtual para aislar las dependencias del proyecto.

```bash
python -m venv venv
```

### Activar el entorno virtual

Una vez creado el entorno virtual, es necesario activarlo para que las dependencias
se instalen y ejecuten Ãºnicamente dentro del proyecto.

#### En Windows
```bash
venv\Scripts\activate
```

#### En macOS o Linux
```bash
source venv/bin/activate
```

---

## ğŸ“¦ Paso 3: Instalar Scrapy

Con el entorno virtual activo, instalamos Scrapy:

```bash
pip install scrapy
```


---

## ğŸ—ï¸ Paso 4: Crear el proyecto Scrapy

Creamos la estructura base del proyecto usando Scrapy:

```bash
scrapy startproject peliculas_tmdb
cd peliculas_tmdb
```



---

## ğŸ•·ï¸ Paso 5: Crear el Spider

Creamos un spider que se conectarÃ¡ a la API de TMDB:

```bash
scrapy genspider productos books.toscrape.com
```


## ğŸ§  Paso 6: CÃ³digo del Spider (scraping HTML)

En este paso definimos la lÃ³gica del spider.
Scrapy descarga el HTML de la pÃ¡gina y extrae los datos usando selectores CSS.

```python
import scrapy

class ProductosSpider(scrapy.Spider):
    name = "productos"
    start_urls = ["https://books.toscrape.com/"]

    def parse(self, response):
        for producto in response.css("article.product_pod"):
            yield {
                "titulo": producto.css("h3 a::attr(title)").get(),
                "precio": producto.css("p.price_color::text").get()
            }

```
En este cÃ³digo:

- Se accede al HTML de la pÃ¡gina
- Se seleccionan los elementos con CSS
- Se extraen los datos visibles al usuario

---

## â–¶ï¸ Paso 7: Ejecutar el Spider

Ejecutamos el spider desde la carpeta donde se encuentra el archivo `scrapy.cfg`:

```bash
scrapy crawl productos -o productos.json

```


---

## ğŸ“„ Resultado

Se genera un archivo productos.json que contiene:
- Nombre del producto
- Precio
- 
Los datos se obtienen directamente desde el HTML de la pÃ¡gina web.
